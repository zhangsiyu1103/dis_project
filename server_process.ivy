#lang ivy1.8
include table

process server(self:server_id) = {

    # Each server process has a socket on the
    # client-server overlay *and* a socket on the
    # management overlay.

    instance sock : net.socket
    instance man_sock : man_net.socket

    # The state of a server consists of:

    instance table : hash_table(hash_t,kvt,shard_t)     # key/value replica
    var view : nat                                      # the current view
    instance confmap : delegation_map(hash_t,config)    # peer servers' configurations
    instance timer : timeout_sec                        # timer for ping messages
    instance man_msg_queue : unbounded_queue(man_msg_t) # queue for manager msgs
    var replicating : bool                              # Are we replicating any keys (used in failover)?


    var re_locked_lo : hash_t                               # low of locked hash range during repartition
    var re_locked_hi : hash_t                               # high of locked hash range during repartition
    instance re_pending_reqs_queue : unbounded_queue(msg_t) # pending requests due to a repartition is ongoing
    var freq : nat                                          # number of read/write served since reset
    
    # Notice here the intial primary and secondary are
    # 0,1, as in the manager.
    
    # TODO Change new init
    after init {
        var conf : config;
        conf.primary := 0;
        conf.secondary := 1;
        conf.replicating := false;
        confmap.set(key_t.iter.begin,key_t.iter.end,conf);
        view := 0;
        replicating := false;

        re_locked_lo := 0;
        re_locked_hi := 0;
        freq := 0;
    }
    
    # Implementation of message receipt on the client-server network.

    # TODO examine code, add homework5 reroute
    # Clean frequency if server moves on the ring

    implement sock.recv(src:tcp.endpoint,msg:msg_t) {
        debug "recv" with self = self, msg = msg;

        # TODO: We need to change the incoming message handling logic.
        # Be careful to make sure that the secondary rejects any
        # incoming requests that are *not* from the current primary.
        # HINT: You can test whether the message comes from server
        # `x` with the expression `src = server(x).sock.id`.

        # If we get an append request and we are primary,
        # do the append and pass it to the secondary.
        # TRICKY: The primary records all the append requests
        # in a ghost queue by calling `serialize`. The secondary commits
        # operations from this queue. However, if the secondary fails,
        # the primary becomes (momentarily) last in the chain, so it
        # must commit the remaining operations in the queue. It's OK for the
        # queue to be shared between processes because it is "ghost" -- it's
        # not actually present in production. 
        # If we are secondary, do the append and send a
        # response message back to the *primary*. In addition, if
        # we are secondary, we 'commit' the operation with
        # a ghost call. 
        msg.tcp_src := src
        process_msg(msg)
    }

    # Return true if mid is in [lo, hi), with hash_t.max + 1 wrapped around to 0.
    function hash_in_between(lo, mid, hi : hash_t) = {
        if lo = hi {
            return false;
        } else if lo < hi {
            return mid >= lo & mid < hi;
        } else {
            return (mid >= lo & mid <= hash_t.max) | (mid >= 0 & mid < hi)
        }
    }

    # Logic for a server to process an incoming message from the client-server overlay network
    action process_msg(msg:msg_t) = {
        # queue up any incoming new request if the request is within the blocked hash range 
        # because a repartition prepare is undertaking.
        if (msg.kind = append_req_kind | msg.kind = show_req_kind) & hash_in_between(re_locked_lo, hash_value(msg.key), re_locked_hi) {
            re_pending_reqs_queue.enqueue(msg);
            return;
        }

        # Otherwise, process it as usual
        freq := freq + 1;

        if msg.kind = append_req_kind {
            var conf := confmap.get(msg.key);  # NEW: look up primary/secondary
            if conf.primary = self {
                var contents_kv := table.get(msg.hash);
                var contents := contents.kv.get(kvt, msg.key, 0);
                contents_kv.set(kvt, msg.key, contents.append(msg.val));
                table.set(msg.hash,contents_kv);
                #table.set(msg.key,contents.append(msg.val));  # NEW: store in table
                sock.send(server(conf.secondary).sock.id,msg);
                serialize(msg.key,self,conf.secondary,msg.src_client);    # ghost call
            }
            else if conf.secondary = self  & ~conf.replicating {  # Note -- ignore if not replicating (could be old!)
                if msg.tcp_src = server(conf.primary).sock.id { 
                    var contents_kv := table.get(msg.hash);
                    var contents := contents.kv.get(kvt, msg.key, 0);
                    contents_kv.set(kvt, msg.key, contents.append(msg.val));
                    table.set(msg.hash,contents_kv);
                    debug "set table" with key=msg.key, val=contents.append(msg.val);
                    msg.kind := append_rsp_kind;
                    sock.send(server(conf.primary).sock.id,msg);  # respond back to server
                    debug "committing" with client = msg.src_client, server = self, kind = msg.kind;
                    commit_one(msg.key);
                }
            }
        } else if msg.kind = append_rsp_kind { 
            sock.send(client(msg.src_client).sock.id,msg);  # We pass it on to client.
        } else if msg.kind = show_req_kind {

            # We now handle show requests at the primary. We respond to the
            # client with a `show_rsp` messages and our current contents. We
            # also commit the operation. Notice if we are waiting for a replicate
            # message, we can't serve `show` requests yet. We could queue the lost
            # requests, but it is just as well to drop them, since the request could have gone
            # to the failed server and been lost anyway.

            var conf := confmap.get(msg.hash);
            if conf.primary = self {
                sock.send(server(conf.secondary).sock.id,msg);
                serialize(msg.key,self,conf.secondary,msg.src_client);   # ghost -- queue for commit
            }
            else if conf.secondary = self & ~conf.replicating {
                if msg.tcp_src = server(conf.primary).sock.id {
                    msg.kind := show_rsp_kind;
                    msg.contents := table.get(msg.hash).get(kvt, msg.key, 0);
                    sock.send(server(conf.primary).sock.id,msg);
                    debug "committing" with client = msg.src_client, server = self, kind = msg.kind;
                    commit_one(msg.key);
                }
            }
        } else if msg.kind = show_rsp_kind { 
            sock.send(client(msg.src_client).sock.id,msg); # Just forward it to client
        } else if msg.kind = replicate_kind {

            # If we get a `replicate` message it means we are the new secondary (even if
            # we haven't gotten the announcement yet!). Set our contents to the contents
            # of the message.

            var conf := confmap.get(msg.key);
            table.incorporate(msg.shard);   # NEW: incorporate new shard in table
            conf.secondary := self;
            conf.primary := msg.primary;
            conf.replicating := false;   # now we are ready to handle show requests
            confmap.set(msg.shard.lo,msg.shard.hi,conf);  # NEW: update configuration map
            replicating := false;
            while ~man_msg_queue.empty & ~replicating {
                process_man_msg(man_msg_queue.dequeue);
            }
        } else if msg.kind = re_replicate_kind {
            # If we get a "re_replicate_kind" message it means we are the new secondary
            # for some hash range. Incoporate those file to our table and ack to the 
            # manager.
            table.incorporate(msg.shard);
            var ackmsg : man_msg_t;
            ackmsg.kind := re_ack_kind;
            for it,mn in manager_id.iter {
                man_sock.send(manager(mn).sock.id, ackmsg);
            }

        }
    }

    action process_man_msg(msg:man_msg_t) = {
        if msg.kind = fail_kind {
            if msg.view = view+1 { # NEW -- ignore duplicate views
                var conf := confmap.get(msg.lo.val);
                
                if msg.primary = self & conf.secondary ~= msg.secondary {
                    var rmsg : msg_t;
                    rmsg.kind := replicate_kind;
                    debug "get table" with val=table.get(3);
                    rmsg.shard := table.extract_(msg.lo,msg.hi); # NEW: extract shard from table
                    rmsg.primary := self;
                    debug "replicating" with primary = msg.primary, secondary = msg.secondary;
                    sock.send(server(msg.secondary).sock.id,rmsg);
                    var it := msg.lo;
                    while it < msg.hi {
                        commit_all_serialized(it.val,self,conf.secondary);  # ghost call
                        it := it.next;
                    }
                }
                # if we are the new secondary, wait for a replicate message
                if self = msg.secondary & self ~= conf.secondary {
                    conf.replicating := true;
                    replicating := true;
                }
                # Now update our state...
                view := msg.view;
                conf.primary := msg.primary;
                conf.secondary := msg.secondary;
                confmap.set(msg.lo,msg.hi,conf);   # NEW set configuration of range
                debug "new view" with server=self, lo = msg.lo, hi = msg.hi,
                primary=conf.primary, secondary=conf.secondary;
            }
        } else if msg.kind = re_prepare_kind {
            # New primary starts to replicate the affected hash range to 
            # new secondary.
            if self = msg.re_conf.new_p {
                # when the manager announce a repartition prepare message, 
                # the new primary should 
                # 1. block the repartitioned hash range 
                # 2. start replicating content in that range to the new secondary
                re_locked_lo := msg.re_conf.lo;
                re_locked_hi := msg.re_conf.hi;

                var rmsg : msg_t;
                rmsg.kind := re_replicate_kind;
                rmsg.shard := table.extract_(msg.re_conf.lo, msg.re_conf.hi);
                sock.send(server(msg.re_conf.new_s).sock.id, rmsg);        
            }

            # If self is the old primary or the new secondary for the affected hash range,
            # do nothing.

        } else if msg.kind = re_view_kind {
            # The manager announces completion of a repartition and it's time for servers
            # to change the view.
            
            # TODO: update server's confmap to reflect the changed responsibilities.


        } else if msg.kind = re_abort_kind {
            # If an ongoing repartition is aborted by manager, the server unlock the range and 
            # processes queued requests as if nothing has happened.
            re_locked_lo := re_locked_hi;
            while ~re_pending_reqs_queue.empty {
                process_msg(re_pending_reqs_queue.dequeue);
            }
        } else {
            # msg.kind not recognizable; should not happen
            assert(false, "msg.kind is unrecognizable");
        }

        
    }

    implement man_sock.recv(src:tcp.endpoint,msg:man_msg_t) {

        # Here we handle an announcement message on the management overlay.
        # If we are the new primary and there is a new secondary, we send
        # our full contents to the secondary in a `replicate` message.
        if replicating {
            man_msg_queue.enqueue(msg);
        } else {
            process_man_msg(msg);
        }
    }

    
    # On a timer 'tick', we send a 'ping' to the manager
    # with our current view number.

    # TODO ping freq

    implement timer.timeout {
        var msg : man_msg_t;
        msg.kind := ping_kind;
        msg.view := view;
        msg.src := self;
        msg.freq := freq;
        for it,mn in manager_id.iter {
            man_sock.send(manager(mn).sock.id,msg);
        }
    }

    # This ghost code is used to keep track of append requests that
    # have been serialized at the primary, but not yet committed at the
    # secondary.
    
    specification {
        instance serialized(K:key_t) : unbounded_queue(client_id)
        action commit_one(k:key_t) = {
            if ~serialized(k).empty {
                commit(serialized(k).dequeue);
            }
        }
        common {
            var true_primary(K:key_t) : server_id
            
            after init {
                true_primary(K) := 0;
            }
            
            action serialize(k:key_t,primary:server_id,secondary:server_id,src_client:client_id) = {
                if primary = true_primary(k) {
                    debug "serialize" with server = secondary, client = src_client;
                    serialized(secondary,k).enqueue(src_client);
                }
            }
            action commit_all_serialized(k:key_t,self:server_id, secondary:server_id) = {
                while ~serialized(secondary,k).empty
                decreases serialized(secondary,k).tail - serialized(secondary,k).head
                {
                invariant 0 <= pos & pos <= s.kv.end
                invariant lo.between(X,hi) & s.value(X) = 0 -> ~tab.contains(X)
                invariant lo.between(X,hi) & 0 <= Y & Y < pos & s.key_at(Y,X) & s.value(X) ~= 0
                                 -> tab.contains(X) & tab.maps(X,s.value(X))
                invariant ~lo.between(X,hi) -> spec.tab_invar(X,Y)
                # following are object invariants of tab and shouldn't be needed here
                invariant tab.maps(X,Y) & tab.maps(X,Z) -> Y = Z & tab.contains(X)
            {
                var k := s.kv.get_key(pos);
                var d := s.kv.get_value(pos);
                if lo.between(k,hi) & d ~= 0{
                    call tab.set(k,d)
                };
                pos := pos.next
            }
                    var cl := serialized(secondary,k).dequeue;
                    if self = true_primary(k) {
                        commit(cl);
                    }
                }
                true_primary(k) := self;
            }
        }
    }
} with client.shard_t.kvt, client.shard_t.index

}

