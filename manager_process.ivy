#lang ivy1.8
include delmap
include multi_paxos




process manager(self:manager_id)  = {

    # The manager's state has the following elements:

    var view : nat                    # The current view number
    var time : nat                    # The current time in seconds
    var change_amt : nat              # The amount of server moving
    var heard(X:server_id) : nat      # Last time we head from server `X`
    var freqs(X:server_id) : nat      # frequency received from server `X`
    instance confmap : delegation_map(hash_t,config)   # NEW: the configuration map
    var proposed : bool               # Have we proposed a new view?

    var used_server : vector(server_id); # used server in the ring
    var server_confs : vector(server_config)   #server idx and its coresponding primary range
    #var primary_range : vector(Range)   #server idx and its coresponding primary range
    #var secondary_range : vector(Range)  #server idx and its coresponding secondary range

    # There is also a parameter that determines how long to wait without a ping
    # before we conclude a server has failed. Notice this parameter has a default
    # value (2) but we can override it on the command line.

    common {
        paramter server_in_ring : nat = 4
        parameter fail_time : nat = 2
        instance op_t : option(man_msg_t)
    }

    # The manager has a socket on the management network

    instance sock : man_net.socket

    # It also has a timer that 'ticks' once per second

    instance timer : timeout_sec

    # Initially, we have...

    after init {
        view := 0;        # view is zero
        time := 0;        # time is zero
        proposed := false;
        var conf : config;
        var server_conf : server_config;

        # Initialize ring with hash code
        var primary := 0;
        var default_len := (hash_t.max+1)/server_in_ring;

        assert hash_t.max+1 >= server_in_ring;

        used_server = used_server.create(server_in_ring,0)
        server_confs = server_confs.create(server_in_ring, server_conf)
        #secondary_range = secondary_range.create(server_in_ring,{0})

        var s := 0;
        var e := s + default_len-1;

        change_amt := default_len / 4;

        # This may looks confusing because ring idx and server id are the same. All vectors are actually indexed by server idx and id are subject to change
        while primary < server_in_ring{
            used_server(primary) := primary
            #used_server := used_server.append(primary);
            conf.primary := primary;
            conf.secondary := primary + 1;
            if conf.secondary == server_in_ring{
                conf.secondary := 0;
            }

            conf.replicating := false;
            if e > hash_t.max{
                e = hash_t.max;
            }

            # set range
            server_confs(conf.primary).primary_lo := s;
            server_confs(conf.primary).primary_hi := e;
            server_confs(conf.secondary).secondary_lo := s;
            server_confs(conf.secondary).secondary_hi := e;
            # set id
            server_confs(conf.primary).self_id := conf.primary;
            server_confs(conf.primary).relative_secondary := conf.secondary;
            server_confs(conf.secondary).relative_primary := conf.primary;

            confmap.set(s,e,conf);
            primary := primary + 1;
            s := e + 1;
            e := s + default_len-1;
        }
    }

    # We say a server is 'up' if we have heard from it in
    # the last 'fail_time' seconds.

    function is_up(S:server_id) = time <= heard(S) + fail_time


    # TODO: the manager handler a time 'tick' event.

    #TODO:  handle failstop of server
    implement timer.timeout {
        debug "tick" with time = time;
        time := time + 1;         # increment the time
        for it, used_s in used_server{
            if ~is_up(used_s) {
                debug "server failed";
                if some (s:server_id) s ~= used_s & is_up(s) {
                    announce(view+1, it, s)
                }
            }
        }
    }



    # TODO annouce both primary and secondary config

    # Here, we announce a new view to all clients and
    # servers on the management overlay. We construct the
    # announcement message and loop over all clients and
    # servers, sending the message to their management
    # socket. We just propose the new view to Paxos here.
    # We execute the new view on callback from Paxos.

    # NEW: We now have a range of keys to deal with.  The
    # range is represented by a low key iterator and a
    # high key iterator.

    action announce(view:nat, ring_idx, server_idx) = {
        if ~proposed {
            var msg : man_msg_t;
            var s_conf := server_confs(ring_idx);
            if s == used_server(ring_idx) {
                s_conf.fail := false;
            } else {
                s_conf.self_id := server_idx;
                s_conf.fail := true;
            }
            msg.view := view;
            msg.server_conf := s_conf;
            # ask paxos to agree on our view change
            paxos.server.propose(client.manager.op_t.just(msg));
            proposed := true;
        }
    }

    # On callback from Paxos we execute the view
    # change. NEW: THis is tricky because the range of keys to
    # assign may cross over an exiting range boundary.  We
    # have to loop, only assigning up to the next range
    # bondary, until the full range is assigned. The
    # servers depend on the new view ranges not crossing
    # existing boundaries.

    implement paxos.server.decide(inst : paxos.instance_t, op: op_t) {
        debug "decide" with inst=inst, op=op;
        proposed := false;
        if ~op.is_empty {
            var msg := op.contents;

            assign_in_steps(msg);

            var s_conf := msg.server_conf;
            var conf : config;
            conf.primary := s_conf.self_idx;
            conf.secondary := s_conf.relative_secondary;
            confmap.set(s_conf.primary_lo,s_conf.primary_hi,conf);


            conf.primary := s_conf.relative_primary;
            conf.secondary := s_conf.self_idx;
            confmap.set(s_conf.secondary_lo,s_conf.secondary_hi,conf);



            #var hi := msg.hi;
            #while msg.lo < hi {
            #    msg.hi := hi;
            #    var lub := confmap.lub(msg.lo.next);
            #    if lub < msg.hi {
            #        msg.hi := lub;
            #    }
            #    assign_in_steps(msg);
            #    confmap.set(msg.lo,msg.hi,conf);
            #    msg.lo := msg.hi;
            }
        }
    }

    # NEW: Each view change is done in two steps:
    # - First, make the new primary secondary
    # - Then make the secondary primary and replicate to new secondary
    # This lets us handle the view changes asynchronously as in HW #5.
    # The first step is needed only both the primary and secondary are changing.



    # TODO coordinate with server replicating
    action assign_in_steps(msg:man_msg_t) = {
        #var config := confmap.get(msg.lo.val);  # get the old config

        var s_conf := msg.server_conf;


        #if config.primary ~= msg.primary & config.secondary ~= msg.primary {
        #    var pmsg := msg;
        #    pmsg.primary := config.primary;
        #    pmsg.secondary := msg.primary; 
        #    broadcast(pmsg);
        #}
        broadcast(msg);
    }

    action broadcast(msg:man_msg_t) = {
        debug "announce" with msg=msg;
        view := view.next;
        msg.view := view;
        for it,cl in client_id.iter {
            sock.send(client(cl).man_sock.id,msg);
        }
        for it,sv in server_id.iter {
            sock.send(server(sv).man_sock.id,msg);
        }
    }

    # Finally, if the manager recevies a 'ping' from a
    # server with the current view, it records the fact that
    # it heard from that server at the current time.

    # TODO: handle frequency and move server in ring

    implement sock.recv(src:tcp.endpoint,msg:man_msg_t) {
        if msg.view = view {
            heard(msg.src) := time;
            freqs(msg.src) := msg.freq;

            var min_freq : nat
            var max_freq : nat
            var min_idx : server_id
            var max_idx : server_id
            var prev_idx : server_id
            var next_idx : server_id
            var change_idx : server_id

            min_freq := freqs(used_server(0));
            max_freq := freqs(used_server(0));

            for it, used_s in used_server{

                if freqs(used_s) < min_freq{
                    min_freq := freqs(used_s);
                    min_idx := it;
                }

                if freqs(used_s) > max_freq{
                    max_freq := freqs(used_s);
                    max_idx := it;
                }
            }

            if max_idx == 0{
                prev_idx := server_in_ring - 1;
            }else{
                prev_idx := max_idx - 1;
            }

            if max_idx == server_in_ring - 1{
                next_idx := 0;
            }else{
                next_idx := max_idx + 1;
            }

            if freq(prev_idx) < freq(next_idx){
                server_confs(prev_idx).primary_hi := server_confs(prev_idx).primary_hi + change_amt;
                server_confs(max_idx).primary_lo := server_confs(max_idx).primary_lo + change_amt;
                server_confs(max_idx).secondary_hi := server_confs(max_idx).secondary_hi + change_amt;
                change_idx := max_idx;
            }else{
                #change_idx := max_idx;
                server_confs(max_idx).primary_hi := server_confs(prev_idx).primary_hi - change_amt;
                server_confs(next_idx).primary_lo := server_confs(max_idx).primary_lo - change_amt;
                server_confs(next_idx).secondary_hi := server_confs(max_idx).secondary_hi - change_amt;
                change_idx := next_idx;
            }


            if min_freq > 1 & max_freq > 2*min_freq{
                announce(view+1, change_idx, used_server(change_idx))
            }

        }
    }

    # NEW: here is the Multi-Paxos instantiation:

    instance paxos : multi_paxos(manager_id,client.manager.op_t,client.manager.op_t.empty)

} with vector[server_id]
